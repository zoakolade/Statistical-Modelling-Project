{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join the data from Part 1 with the data from Part 2 to create a new dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming you have 'stations', 'foursquare_df', and 'yelp_df' DataFrames\n",
    "import pandas as pd\n",
    "\n",
    "# Merge Foursquare data with station details\n",
    "foursquare_merged = pd.merge(stations, foursquare_df, left_on=['latitude', 'longitude'], right_on=['venue.location.lat', 'venue.location.lng'], how='left')\n",
    "\n",
    "# Merge Yelp data with station details\n",
    "yelp_merged = pd.merge(stations, yelp_df, left_on=['latitude', 'longitude'], right_on=['coordinates.latitude', 'coordinates.longitude'], how='left')\n",
    "\n",
    "# Concatenate both merged DataFrames\n",
    "merged_data = pd.concat([foursquare_merged, yelp_merged], keys=['Foursquare', 'Yelp'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide a visualization that you used as part of your EDA process. Explain the initial pattern or relationship you discoved through this visualization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(merged_data.index.get_level_values(1), merged_data['free_bikes'])\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Station')\n",
    "plt.ylabel('Number of Available Bikes')\n",
    "plt.title('Available Bikes at Each Station')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put all your results in an SQLite3 database (remember, SQLite stores its databases as files in your local machine - make sure to create your database in your project's data/ directory!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create a connection to the database (or connect to an existing one)\n",
    "conn = sqlite3.connect('bike_pois.db')\n",
    "\n",
    "# Create a cursor object\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create a table for POI data\n",
    "cursor.execute('''\n",
    "    CREATE TABLE IF NOT EXISTS pois (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        station_id INTEGER,\n",
    "        api_source TEXT,\n",
    "        poi_name TEXT,\n",
    "        category TEXT,\n",
    "        latitude REAL,\n",
    "        longitude REAL,\n",
    "        num_bikes INTEGER,\n",
    "        rating REAL,\n",
    "        reviews_count INTEGER\n",
    "    )\n",
    "''')\n",
    "\n",
    "# Iterate through the merged data and insert records into the table\n",
    "for index, row in merged_data.iterrows():\n",
    "    cursor.execute('''\n",
    "        INSERT INTO pois (station_id, api_source, poi_name, category, latitude, longitude, num_bikes, rating, reviews_count)\n",
    "        VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "    ''', (index[1], index[0], row['venue.name'], row['venue.categories'][0]['name'], row['latitude_x'], row['longitude_x'], row['free_bikes'], row['venue.rating'] if 'venue.rating' in row else None, row['venue.ratingSignals'] if 'venue.ratingSignals' in row else None))\n",
    "\n",
    "# then I Commit the changes and close the connection\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the data before and after the join to validate your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate entries\n",
    "duplicate_entries = merged_data[merged_data.duplicated(subset=['latitude', 'longitude', 'api_source'])]\n",
    "print(\"Duplicate Entries:\")\n",
    "print(duplicate_entries)\n",
    "\n",
    "# Check for missing values\n",
    "missing_values = merged_data[merged_data.isnull().any(axis=1)]\n",
    "print(\"Missing Values:\")\n",
    "print(missing_values)\n",
    "\n",
    "# Compare counts\n",
    "print(\"Counts:\")\n",
    "print(\"Original Foursquare Count:\", len(foursquare_df))\n",
    "print(\"Original Yelp Count:\", len(yelp_df))\n",
    "print(\"Merged Data Count:\", len(merged_data))\n",
    "\n",
    "# Verify data types\n",
    "print(\"Data Types:\")\n",
    "print(merged_data.dtypes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
